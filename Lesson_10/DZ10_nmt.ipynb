{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Задание:\n",
        "Разобраться с моделькой перевода как она устроена\n",
        "запустить для перевода с русского на английский (при желании можно взять другие пары языков) два варианта с вниманием и без внимания\n",
        "оценить качество насколько корректно переводит (для теста отобрать примеры с увеличением длины текста) (так как оценка визуальная достаточно 20-ти примеров в тестовой выборке)"
      ],
      "metadata": {
        "id": "z9VKxZCk3Nn8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNvjhDyAKk3U",
        "outputId": "8f3787fe-73c5-4aee-9334-7e5e03179d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-02 10:50:06--  http://www.manythings.org/anki/rus-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14819554 (14M) [application/zip]\n",
            "Saving to: ‘rus-eng.zip’\n",
            "\n",
            "rus-eng.zip         100%[===================>]  14.13M  17.2MB/s    in 0.8s    \n",
            "\n",
            "2022-07-02 10:50:07 (17.2 MB/s) - ‘rus-eng.zip’ saved [14819554/14819554]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.manythings.org/anki/rus-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bg17Lr-7XK",
        "outputId": "3c4721d7-953c-4f26-c6ef-8122adba170b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rus-eng.zip\n",
            "  inflating: rus-eng/rus.txt         \n",
            "  inflating: rus-eng/_about.txt      \n"
          ]
        }
      ],
      "source": [
        "#!mkdir rus-eng\n",
        "!unzip rus-eng.zip -d rus-eng/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o5L92efMMhf",
        "outputId": "f877ea03-d33c-47a1-c4f9-e2ae479cbe7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 71M\n",
            "drwxr-xr-x 2 root root 4.0K Jul  2 10:50 .\n",
            "drwxr-xr-x 1 root root 4.0K Jul  2 10:50 ..\n",
            "-rw-r--r-- 1 root root 1.5K May  2 01:29 _about.txt\n",
            "-rw-r--r-- 1 root root  71M May  2 01:29 rus.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/rus-eng/ -lah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "outputs": [],
      "source": [
        "# Download the file\n",
        "path_to_file = \"/content/rus-eng/rus.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower().strip()\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yV9lZXQXNbnH",
        "outputId": "5d9dd120-03fc-49e4-c4ce-23c2995dbcc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start> i can't go . <end>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "preprocess_sentence(\"I can't go.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENG, RUS]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTbSbBz55QtF",
        "outputId": "6a04703c-2c50-4358-9415-f8f573e7fbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> go . <end>\n",
            "<start> марш ! <end>\n"
          ]
        }
      ],
      "source": [
        "en, ru = create_dataset(path_to_file, None)\n",
        "print(en[0])\n",
        "print(ru[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8j9g9AnIeZV",
        "outputId": "a035508d-04b2-430d-95a9-195763d5b1ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(444587, 444587)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(en), len(ru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cnxC7q-j3jFD"
      },
      "outputs": [],
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-GHoBo00ZfK",
        "outputId": "0b8d1a3c-188b-44d8-a1b0-263d3bc006ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f528bcd3ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "inp_lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fmuMjRr0NOz",
        "outputId": "39c1f49a-7a97-4cd5-ed8c-af55afc2de23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 5674,   24,    2,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "input_tensor[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QILQkOs3jFG",
        "outputId": "5ee70984-418e-423a-fe63-ad06832a8c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80000 80000 20000 20000\n"
          ]
        }
      ],
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "outputs": [],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXukARTDd7MT",
        "outputId": "863a2c76-70d2-4200-a0de-74ea2b7d2544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> я\n",
            "9555 ----> струсил\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "3293 ----> chickened\n",
            "86 ----> out\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 300\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "#dataset = dataset.batch(BATCH_SIZE, drop_remainder==True)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc6-NK1GtWQt",
        "outputId": "0672cecc-89d9-40b4-c854-35c0d1a658a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 15]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    #self.bigru_0 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.enc_units, \n",
        "    #                                            return_sequences=True, return_state=True))\n",
        "    \n",
        "    self.bigru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.enc_units, \n",
        "                                                return_sequences=False, return_state=True))\n",
        "\n",
        "\n",
        "    #self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "    #                               return_sequences=False,\n",
        "    #                               return_state=True,\n",
        "    #                               recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    #state = self.bigru_0(x, initial_state = hidden)\n",
        "    #state = self.bigru(state)\n",
        "\n",
        "    state = self.bigru(x, initial_state = hidden)\n",
        "    #output, state = self.gru(x, initial_state = hidden)\n",
        "    return state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)) for i in range(2)]\n",
        "    #return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gSVh05Jl6l",
        "outputId": "80c8dba8-4e4c-490b-9b01-a099de7f4596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Hidden state shape: (batch size, units) [<tf.Tensor: shape=(64, 2048), dtype=float32, numpy=\n",
            "array([[-2.0716032e-03,  1.3614398e-02,  5.3938883e-03, ...,\n",
            "        -7.3685851e-03, -4.1158078e-03,  8.6220708e-03],\n",
            "       [-2.0446840e-03,  1.3694559e-02,  5.3544156e-03, ...,\n",
            "        -4.0086787e-03, -1.8176429e-03,  6.6663893e-03],\n",
            "       [-2.0065152e-03,  1.3695389e-02,  5.3953752e-03, ...,\n",
            "        -1.2506780e-02, -4.4213586e-05,  1.1518727e-03],\n",
            "       ...,\n",
            "       [-2.0737289e-03,  1.3652696e-02,  5.3830408e-03, ...,\n",
            "        -1.0708647e-02,  1.6229097e-03,  7.8544365e-03],\n",
            "       [-2.1510720e-03,  1.3698903e-02,  5.2846023e-03, ...,\n",
            "        -8.0346745e-03, -2.8387839e-03,  4.8408457e-03],\n",
            "       [-2.1159113e-03,  1.3731044e-02,  5.4212408e-03, ...,\n",
            "        -2.0975992e-03,  7.5553562e-03,  5.8971317e-03]], dtype=float32)>, <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
            "array([[-0.0020716 ,  0.0136144 ,  0.00539389, ..., -0.00956457,\n",
            "         0.01592911, -0.01610116],\n",
            "       [-0.00204468,  0.01369456,  0.00535442, ..., -0.00949592,\n",
            "         0.01592408, -0.01613946],\n",
            "       [-0.00200652,  0.01369539,  0.00539538, ..., -0.00954573,\n",
            "         0.01593249, -0.016168  ],\n",
            "       ...,\n",
            "       [-0.00207373,  0.0136527 ,  0.00538304, ..., -0.00960424,\n",
            "         0.0158805 , -0.01610729],\n",
            "       [-0.00215107,  0.0136989 ,  0.0052846 , ..., -0.00931449,\n",
            "         0.01576513, -0.01621801],\n",
            "       [-0.00211591,  0.01373104,  0.00542124, ..., -0.00934921,\n",
            "         0.01582977, -0.01621302]], dtype=float32)>, <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
            "array([[-1.1303568e-02, -2.5471814e-03, -3.3189533e-03, ...,\n",
            "        -7.3685851e-03, -4.1158078e-03,  8.6220708e-03],\n",
            "       [-6.7544063e-03, -5.8639944e-03, -2.8140999e-03, ...,\n",
            "        -4.0086787e-03, -1.8176429e-03,  6.6663893e-03],\n",
            "       [-8.7270513e-03, -1.0616530e-02,  4.1144164e-03, ...,\n",
            "        -1.2506780e-02, -4.4213586e-05,  1.1518727e-03],\n",
            "       ...,\n",
            "       [-4.6272632e-03, -1.0540173e-02, -2.7061487e-03, ...,\n",
            "        -1.0708647e-02,  1.6229097e-03,  7.8544365e-03],\n",
            "       [-4.1347519e-03, -4.6097171e-03,  7.4529722e-03, ...,\n",
            "        -8.0346745e-03, -2.8387839e-03,  4.8408457e-03],\n",
            "       [-8.1436196e-03, -7.9105645e-03, -5.1496010e-03, ...,\n",
            "        -2.0975992e-03,  7.5553562e-03,  5.8971317e-03]], dtype=float32)>]\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "encoder_states = encoder.initialize_hidden_state()\n",
        "#state_c = encoder.initialize_hidden_state()\n",
        "#encoder_states = [state_h, state_c]\n",
        "\n",
        "# sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_hidden = encoder(example_input_batch, encoder_states)\n",
        "\n",
        "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_hidden[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCbmBEm_pbyG",
        "outputId": "f46c8e19-2385-48ab-e24b-fa52950dc4fe"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 2048), dtype=float32, numpy=\n",
              "array([[-2.0716032e-03,  1.3614398e-02,  5.3938883e-03, ...,\n",
              "        -7.3685851e-03, -4.1158078e-03,  8.6220708e-03],\n",
              "       [-2.0446840e-03,  1.3694559e-02,  5.3544156e-03, ...,\n",
              "        -4.0086787e-03, -1.8176429e-03,  6.6663893e-03],\n",
              "       [-2.0065152e-03,  1.3695389e-02,  5.3953752e-03, ...,\n",
              "        -1.2506780e-02, -4.4213586e-05,  1.1518727e-03],\n",
              "       ...,\n",
              "       [-2.0737289e-03,  1.3652696e-02,  5.3830408e-03, ...,\n",
              "        -1.0708647e-02,  1.6229097e-03,  7.8544365e-03],\n",
              "       [-2.1510720e-03,  1.3698903e-02,  5.2846023e-03, ...,\n",
              "        -8.0346745e-03, -2.8387839e-03,  4.8408457e-03],\n",
              "       [-2.1159113e-03,  1.3731044e-02,  5.4212408e-03, ...,\n",
              "        -2.0975992e-03,  7.5553562e-03,  5.8971317e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_hidden[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30HLwEXkpiEy",
        "outputId": "f31a4fd4-91a8-46f5-c665-be72ee71052b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              "array([[-0.00651659,  0.0015186 ,  0.00477847, ...,  0.00076088,\n",
              "        -0.00164839,  0.00754175],\n",
              "       [-0.00691843,  0.0016113 ,  0.00455411, ...,  0.00080611,\n",
              "        -0.00164456,  0.00777414],\n",
              "       [-0.0069491 ,  0.00160151,  0.00460188, ...,  0.00071912,\n",
              "        -0.00172119,  0.00768685],\n",
              "       ...,\n",
              "       [-0.00744378,  0.00173575,  0.00437781, ...,  0.00074614,\n",
              "        -0.00163993,  0.00778439],\n",
              "       [-0.00695033,  0.00159173,  0.00471702, ...,  0.0007499 ,\n",
              "        -0.00156053,  0.00777683],\n",
              "       [-0.00725776,  0.00176134,  0.0044791 , ...,  0.00074841,\n",
              "        -0.00161041,  0.00776347]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XKcypC0AGeLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d12e89-5374-406d-8927-7a430369d49a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 7334])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "decoder_sample_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6y0HF-zMF_vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea0daf91-8d87-403d-a961-89105ab77011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "decoder_sample_h.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_nmt_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bUFF_LhEyZQd"
      },
      "outputs": [],
      "source": [
        "# <s> w1 w2 w3 w4 <end>\n",
        "# 0    t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    #dec_hidden = enc_hidden\n",
        "    dec_hidden = enc_hidden[1]\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf2b2d2-3a8c-467b-a62d-70548eab7e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bidirectional/backward_gru/gru_cell_2/kernel:0', 'encoder/bidirectional/backward_gru/gru_cell_2/recurrent_kernel:0', 'encoder/bidirectional/backward_gru/gru_cell_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['encoder/bidirectional/backward_gru/gru_cell_2/kernel:0', 'encoder/bidirectional/backward_gru/gru_cell_2/recurrent_kernel:0', 'encoder/bidirectional/backward_gru/gru_cell_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "Epoch 1 Batch 0 Loss 4.7150\n",
            "Epoch 1 Batch 100 Loss 2.0174\n",
            "Epoch 1 Batch 200 Loss 1.8183\n",
            "Epoch 1 Batch 300 Loss 1.5324\n",
            "Epoch 1 Batch 400 Loss 1.5406\n",
            "Epoch 1 Batch 500 Loss 1.5446\n",
            "Epoch 1 Batch 600 Loss 1.4341\n",
            "Epoch 1 Batch 700 Loss 1.3243\n",
            "Epoch 1 Batch 800 Loss 1.2832\n",
            "Epoch 1 Batch 900 Loss 1.2192\n",
            "Epoch 1 Batch 1000 Loss 1.1860\n",
            "Epoch 1 Batch 1100 Loss 1.1526\n",
            "Epoch 1 Batch 1200 Loss 1.1915\n",
            "Epoch 1 Loss 1.4937\n",
            "Time taken for 1 epoch 67.18843007087708 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.9780\n",
            "Epoch 2 Batch 100 Loss 0.9951\n",
            "Epoch 2 Batch 200 Loss 0.8783\n",
            "Epoch 2 Batch 300 Loss 0.9091\n",
            "Epoch 2 Batch 400 Loss 0.8119\n",
            "Epoch 2 Batch 500 Loss 0.9260\n",
            "Epoch 2 Batch 600 Loss 0.8656\n",
            "Epoch 2 Batch 700 Loss 0.7707\n",
            "Epoch 2 Batch 800 Loss 0.6817\n",
            "Epoch 2 Batch 900 Loss 0.6837\n",
            "Epoch 2 Batch 1000 Loss 0.5662\n",
            "Epoch 2 Batch 1100 Loss 0.6232\n",
            "Epoch 2 Batch 1200 Loss 0.6124\n",
            "Epoch 2 Loss 0.7957\n",
            "Time taken for 1 epoch 58.21935558319092 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5294\n",
            "Epoch 3 Batch 100 Loss 0.4274\n",
            "Epoch 3 Batch 200 Loss 0.5015\n",
            "Epoch 3 Batch 300 Loss 0.3540\n",
            "Epoch 3 Batch 400 Loss 0.5458\n",
            "Epoch 3 Batch 500 Loss 0.4529\n",
            "Epoch 3 Batch 600 Loss 0.5349\n",
            "Epoch 3 Batch 700 Loss 0.4444\n",
            "Epoch 3 Batch 800 Loss 0.4031\n",
            "Epoch 3 Batch 900 Loss 0.3795\n",
            "Epoch 3 Batch 1000 Loss 0.4985\n",
            "Epoch 3 Batch 1100 Loss 0.4125\n",
            "Epoch 3 Batch 1200 Loss 0.3824\n",
            "Epoch 3 Loss 0.4434\n",
            "Time taken for 1 epoch 58.177911043167114 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2727\n",
            "Epoch 4 Batch 100 Loss 0.1917\n",
            "Epoch 4 Batch 200 Loss 0.2352\n",
            "Epoch 4 Batch 300 Loss 0.2268\n",
            "Epoch 4 Batch 400 Loss 0.2883\n",
            "Epoch 4 Batch 500 Loss 0.2249\n",
            "Epoch 4 Batch 600 Loss 0.2191\n",
            "Epoch 4 Batch 700 Loss 0.3161\n",
            "Epoch 4 Batch 800 Loss 0.2851\n",
            "Epoch 4 Batch 900 Loss 0.2886\n",
            "Epoch 4 Batch 1000 Loss 0.3061\n",
            "Epoch 4 Batch 1100 Loss 0.2891\n",
            "Epoch 4 Batch 1200 Loss 0.2712\n",
            "Epoch 4 Loss 0.2597\n",
            "Time taken for 1 epoch 58.72459650039673 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2043\n",
            "Epoch 5 Batch 100 Loss 0.1838\n",
            "Epoch 5 Batch 200 Loss 0.1190\n",
            "Epoch 5 Batch 300 Loss 0.1451\n",
            "Epoch 5 Batch 400 Loss 0.1838\n",
            "Epoch 5 Batch 500 Loss 0.1718\n",
            "Epoch 5 Batch 600 Loss 0.1615\n",
            "Epoch 5 Batch 700 Loss 0.1768\n",
            "Epoch 5 Batch 800 Loss 0.2082\n",
            "Epoch 5 Batch 900 Loss 0.2299\n",
            "Epoch 5 Batch 1000 Loss 0.2153\n",
            "Epoch 5 Batch 1100 Loss 0.2365\n",
            "Epoch 5 Batch 1200 Loss 0.2434\n",
            "Epoch 5 Loss 0.1702\n",
            "Time taken for 1 epoch 58.16988158226013 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0882\n",
            "Epoch 6 Batch 100 Loss 0.1074\n",
            "Epoch 6 Batch 200 Loss 0.1607\n",
            "Epoch 6 Batch 300 Loss 0.1190\n",
            "Epoch 6 Batch 400 Loss 0.1652\n",
            "Epoch 6 Batch 500 Loss 0.1249\n",
            "Epoch 6 Batch 600 Loss 0.1350\n",
            "Epoch 6 Batch 700 Loss 0.1489\n",
            "Epoch 6 Batch 800 Loss 0.1214\n",
            "Epoch 6 Batch 900 Loss 0.1364\n",
            "Epoch 6 Batch 1000 Loss 0.1112\n",
            "Epoch 6 Batch 1100 Loss 0.1643\n",
            "Epoch 6 Batch 1200 Loss 0.1366\n",
            "Epoch 6 Loss 0.1267\n",
            "Time taken for 1 epoch 58.857492446899414 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0559\n",
            "Epoch 7 Batch 100 Loss 0.0840\n",
            "Epoch 7 Batch 200 Loss 0.1071\n",
            "Epoch 7 Batch 300 Loss 0.0786\n",
            "Epoch 7 Batch 400 Loss 0.0865\n",
            "Epoch 7 Batch 500 Loss 0.1095\n",
            "Epoch 7 Batch 600 Loss 0.0884\n",
            "Epoch 7 Batch 700 Loss 0.1220\n",
            "Epoch 7 Batch 800 Loss 0.1191\n",
            "Epoch 7 Batch 900 Loss 0.1129\n",
            "Epoch 7 Batch 1000 Loss 0.1302\n",
            "Epoch 7 Batch 1100 Loss 0.1664\n",
            "Epoch 7 Batch 1200 Loss 0.1044\n",
            "Epoch 7 Loss 0.1039\n",
            "Time taken for 1 epoch 58.25843048095703 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0916\n",
            "Epoch 8 Batch 100 Loss 0.0902\n",
            "Epoch 8 Batch 200 Loss 0.0869\n",
            "Epoch 8 Batch 300 Loss 0.0587\n",
            "Epoch 8 Batch 400 Loss 0.0978\n",
            "Epoch 8 Batch 500 Loss 0.0769\n",
            "Epoch 8 Batch 600 Loss 0.1170\n",
            "Epoch 8 Batch 700 Loss 0.0731\n",
            "Epoch 8 Batch 800 Loss 0.0923\n",
            "Epoch 8 Batch 900 Loss 0.1080\n",
            "Epoch 8 Batch 1000 Loss 0.0732\n",
            "Epoch 8 Batch 1100 Loss 0.1010\n",
            "Epoch 8 Batch 1200 Loss 0.1127\n",
            "Epoch 8 Loss 0.0916\n",
            "Time taken for 1 epoch 58.80075216293335 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0607\n",
            "Epoch 9 Batch 100 Loss 0.0670\n",
            "Epoch 9 Batch 200 Loss 0.1099\n",
            "Epoch 9 Batch 300 Loss 0.0918\n",
            "Epoch 9 Batch 400 Loss 0.1142\n",
            "Epoch 9 Batch 500 Loss 0.1112\n",
            "Epoch 9 Batch 600 Loss 0.1086\n",
            "Epoch 9 Batch 700 Loss 0.0795\n",
            "Epoch 9 Batch 800 Loss 0.0927\n",
            "Epoch 9 Batch 900 Loss 0.0967\n",
            "Epoch 9 Batch 1000 Loss 0.0824\n",
            "Epoch 9 Batch 1100 Loss 0.1498\n",
            "Epoch 9 Batch 1200 Loss 0.0820\n",
            "Epoch 9 Loss 0.0845\n",
            "Time taken for 1 epoch 58.29475545883179 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0525\n",
            "Epoch 10 Batch 100 Loss 0.0917\n",
            "Epoch 10 Batch 200 Loss 0.0754\n",
            "Epoch 10 Batch 300 Loss 0.0508\n",
            "Epoch 10 Batch 400 Loss 0.0746\n",
            "Epoch 10 Batch 500 Loss 0.0760\n",
            "Epoch 10 Batch 600 Loss 0.0848\n",
            "Epoch 10 Batch 700 Loss 0.0749\n",
            "Epoch 10 Batch 800 Loss 0.0673\n",
            "Epoch 10 Batch 900 Loss 0.0766\n",
            "Epoch 10 Batch 1000 Loss 0.0987\n",
            "Epoch 10 Batch 1100 Loss 0.1242\n",
            "Epoch 10 Batch 1200 Loss 0.1120\n",
            "Epoch 10 Loss 0.0799\n",
            "Time taken for 1 epoch 58.88393998146057 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0509\n",
            "Epoch 11 Batch 100 Loss 0.0714\n",
            "Epoch 11 Batch 200 Loss 0.0836\n",
            "Epoch 11 Batch 300 Loss 0.0753\n",
            "Epoch 11 Batch 400 Loss 0.0617\n",
            "Epoch 11 Batch 500 Loss 0.0696\n",
            "Epoch 11 Batch 600 Loss 0.0876\n",
            "Epoch 11 Batch 700 Loss 0.0715\n",
            "Epoch 11 Batch 800 Loss 0.0939\n",
            "Epoch 11 Batch 900 Loss 0.0747\n",
            "Epoch 11 Batch 1000 Loss 0.0623\n",
            "Epoch 11 Batch 1100 Loss 0.0688\n",
            "Epoch 11 Batch 1200 Loss 0.0449\n",
            "Epoch 11 Loss 0.0758\n",
            "Time taken for 1 epoch 58.33455514907837 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0751\n",
            "Epoch 12 Batch 100 Loss 0.0502\n",
            "Epoch 12 Batch 200 Loss 0.0657\n",
            "Epoch 12 Batch 300 Loss 0.0433\n",
            "Epoch 12 Batch 400 Loss 0.0889\n",
            "Epoch 12 Batch 500 Loss 0.0654\n",
            "Epoch 12 Batch 600 Loss 0.0970\n",
            "Epoch 12 Batch 700 Loss 0.0734\n",
            "Epoch 12 Batch 800 Loss 0.0836\n",
            "Epoch 12 Batch 900 Loss 0.1016\n",
            "Epoch 12 Batch 1000 Loss 0.0623\n",
            "Epoch 12 Batch 1100 Loss 0.0979\n",
            "Epoch 12 Batch 1200 Loss 0.1080\n",
            "Epoch 12 Loss 0.0728\n",
            "Time taken for 1 epoch 58.9076406955719 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0515\n",
            "Epoch 13 Batch 100 Loss 0.0589\n",
            "Epoch 13 Batch 200 Loss 0.0586\n",
            "Epoch 13 Batch 300 Loss 0.0587\n",
            "Epoch 13 Batch 400 Loss 0.0978\n",
            "Epoch 13 Batch 500 Loss 0.0665\n",
            "Epoch 13 Batch 600 Loss 0.0744\n",
            "Epoch 13 Batch 700 Loss 0.0465\n",
            "Epoch 13 Batch 800 Loss 0.1124\n",
            "Epoch 13 Batch 900 Loss 0.0573\n",
            "Epoch 13 Batch 1000 Loss 0.0889\n",
            "Epoch 13 Batch 1100 Loss 0.0817\n",
            "Epoch 13 Batch 1200 Loss 0.0869\n",
            "Epoch 13 Loss 0.0701\n",
            "Time taken for 1 epoch 58.25300216674805 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0827\n",
            "Epoch 14 Batch 100 Loss 0.0257\n",
            "Epoch 14 Batch 200 Loss 0.0403\n",
            "Epoch 14 Batch 300 Loss 0.0513\n",
            "Epoch 14 Batch 400 Loss 0.0376\n",
            "Epoch 14 Batch 500 Loss 0.0537\n",
            "Epoch 14 Batch 600 Loss 0.0325\n",
            "Epoch 14 Batch 700 Loss 0.0604\n",
            "Epoch 14 Batch 800 Loss 0.0794\n",
            "Epoch 14 Batch 900 Loss 0.0484\n",
            "Epoch 14 Batch 1000 Loss 0.0875\n",
            "Epoch 14 Batch 1100 Loss 0.0849\n",
            "Epoch 14 Batch 1200 Loss 0.0953\n",
            "Epoch 14 Loss 0.0680\n",
            "Time taken for 1 epoch 59.29143571853638 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0466\n",
            "Epoch 15 Batch 100 Loss 0.0630\n",
            "Epoch 15 Batch 200 Loss 0.0406\n",
            "Epoch 15 Batch 300 Loss 0.0358\n",
            "Epoch 15 Batch 400 Loss 0.0695\n",
            "Epoch 15 Batch 500 Loss 0.0606\n",
            "Epoch 15 Batch 600 Loss 0.0766\n",
            "Epoch 15 Batch 700 Loss 0.0700\n",
            "Epoch 15 Batch 800 Loss 0.0618\n",
            "Epoch 15 Batch 900 Loss 0.0808\n",
            "Epoch 15 Batch 1000 Loss 0.0788\n",
            "Epoch 15 Batch 1100 Loss 0.0674\n",
            "Epoch 15 Batch 1200 Loss 0.0885\n",
            "Epoch 15 Loss 0.0669\n",
            "Time taken for 1 epoch 58.44794797897339 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0500\n",
            "Epoch 16 Batch 100 Loss 0.0708\n",
            "Epoch 16 Batch 200 Loss 0.0473\n",
            "Epoch 16 Batch 300 Loss 0.0994\n",
            "Epoch 16 Batch 400 Loss 0.0516\n",
            "Epoch 16 Batch 500 Loss 0.0847\n",
            "Epoch 16 Batch 600 Loss 0.0467\n",
            "Epoch 16 Batch 700 Loss 0.0631\n",
            "Epoch 16 Batch 800 Loss 0.0624\n",
            "Epoch 16 Batch 900 Loss 0.0982\n",
            "Epoch 16 Batch 1000 Loss 0.0425\n",
            "Epoch 16 Batch 1100 Loss 0.1072\n",
            "Epoch 16 Batch 1200 Loss 0.0738\n",
            "Epoch 16 Loss 0.0651\n",
            "Time taken for 1 epoch 59.163753032684326 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0430\n",
            "Epoch 17 Batch 100 Loss 0.0498\n",
            "Epoch 17 Batch 200 Loss 0.1031\n",
            "Epoch 17 Batch 300 Loss 0.0706\n",
            "Epoch 17 Batch 400 Loss 0.0578\n",
            "Epoch 17 Batch 500 Loss 0.0784\n",
            "Epoch 17 Batch 600 Loss 0.1296\n",
            "Epoch 17 Batch 700 Loss 0.0608\n",
            "Epoch 17 Batch 800 Loss 0.0854\n",
            "Epoch 17 Batch 900 Loss 0.0830\n",
            "Epoch 17 Batch 1000 Loss 0.0943\n",
            "Epoch 17 Batch 1100 Loss 0.0839\n",
            "Epoch 17 Batch 1200 Loss 0.0575\n",
            "Epoch 17 Loss 0.0634\n",
            "Time taken for 1 epoch 58.6875479221344 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0427\n",
            "Epoch 18 Batch 100 Loss 0.0726\n",
            "Epoch 18 Batch 200 Loss 0.0510\n",
            "Epoch 18 Batch 300 Loss 0.0808\n",
            "Epoch 18 Batch 400 Loss 0.0465\n",
            "Epoch 18 Batch 500 Loss 0.0275\n",
            "Epoch 18 Batch 600 Loss 0.0719\n",
            "Epoch 18 Batch 700 Loss 0.0508\n",
            "Epoch 18 Batch 800 Loss 0.0778\n",
            "Epoch 18 Batch 900 Loss 0.0451\n",
            "Epoch 18 Batch 1000 Loss 0.0480\n",
            "Epoch 18 Batch 1100 Loss 0.0862\n",
            "Epoch 18 Batch 1200 Loss 0.0301\n",
            "Epoch 18 Loss 0.0624\n",
            "Time taken for 1 epoch 59.28549003601074 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0565\n",
            "Epoch 19 Batch 100 Loss 0.0271\n",
            "Epoch 19 Batch 200 Loss 0.0546\n",
            "Epoch 19 Batch 300 Loss 0.1034\n",
            "Epoch 19 Batch 400 Loss 0.0681\n",
            "Epoch 19 Batch 500 Loss 0.0888\n",
            "Epoch 19 Batch 600 Loss 0.0746\n",
            "Epoch 19 Batch 700 Loss 0.0981\n",
            "Epoch 19 Batch 800 Loss 0.0527\n",
            "Epoch 19 Batch 900 Loss 0.0939\n",
            "Epoch 19 Batch 1000 Loss 0.0414\n",
            "Epoch 19 Batch 1100 Loss 0.0594\n",
            "Epoch 19 Batch 1200 Loss 0.0600\n",
            "Epoch 19 Loss 0.0617\n",
            "Time taken for 1 epoch 58.347172021865845 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0370\n",
            "Epoch 20 Batch 100 Loss 0.0497\n",
            "Epoch 20 Batch 200 Loss 0.0416\n",
            "Epoch 20 Batch 300 Loss 0.0529\n",
            "Epoch 20 Batch 400 Loss 0.0360\n",
            "Epoch 20 Batch 500 Loss 0.0766\n",
            "Epoch 20 Batch 600 Loss 0.0492\n",
            "Epoch 20 Batch 700 Loss 0.0545\n",
            "Epoch 20 Batch 800 Loss 0.1001\n",
            "Epoch 20 Batch 900 Loss 0.0727\n",
            "Epoch 20 Batch 1000 Loss 0.0299\n",
            "Epoch 20 Batch 1100 Loss 0.0971\n",
            "Epoch 20 Batch 1200 Loss 0.1044\n",
            "Epoch 20 Loss 0.0596\n",
            "Time taken for 1 epoch 58.961103439331055 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  #hidden = [tf.zeros((1, units))]\n",
        "  hidden = [tf.zeros((1, units)) for i in range(2)]\n",
        "\n",
        "  enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  #dec_hidden = enc_hidden\n",
        "  dec_hidden = enc_hidden[1]\n",
        "\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4cf92b-dd6b-487b-b8fa-50fdced92aef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5275c41c50>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WrAM0FDomq3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3e7fd9-df8a-4c80-c5fb-682dc798b5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> здесь хорошо . <end>\n",
            "Predicted translation: it's ok here . <end> \n"
          ]
        }
      ],
      "source": [
        "translate('Здесь хорошо.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5bhFfwcIMX5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff77f86-350a-4828-f75f-7d98f81b8fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> я не смогу поехать . <end>\n",
            "Predicted translation: i can't go . <end> \n"
          ]
        }
      ],
      "source": [
        "translate('Я не смогу поехать.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zSx2iM36EZQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65fa04d-bdaa-48a4-83fc-97d4855e53a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> вы еще дома ? <end>\n",
            "Predicted translation: are you still at home ? <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Вы еще дома?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "A3LLCx3ZE0Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee570ca2-ffd1-49dc-ae19-d872ea50e70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> вы все еще дома ? <end>\n",
            "Predicted translation: are you still at home ? <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Вы все еще дома?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DUQVLVqUE1YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168b331a-1996-41ca-ae5d-6857e1d6402d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> попробуй сделать это . <end>\n",
            "Predicted translation: try to do that . <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Попробуй сделать это.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "f09_hUFx9EJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a328ba7a-abf0-4a58-a090-8aa51778cb70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> я люблю , когда идет снег . <end>\n",
            "Predicted translation: i like to try it . <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Я люблю, когда идет снег.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "e7c5p8rmkHQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a114727-d041-4bed-a11c-2de94f5bf91b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> я никогда такого не делаю . <end>\n",
            "Predicted translation: i never do that . <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Я никогда такого не делаю.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'За плечами у меня не было ни опыта работы')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Jxxt7rllaa",
        "outputId": "882b9612-4b42-425b-83c2-b14f26813f62"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> за плечами у меня не было ни опыта работы <end>\n",
            "Predicted translation: i ran about tom's . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'За плечами у меня не было ни опыта работы, ни подготовки.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "SNeYUNEbllga",
        "outputId": "58ccdfd8-4fbb-4edf-fd7e-70ed12eea10f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-37b3fc89a16f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'За плечами у меня не было ни опыта работы, ни подготовки.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-d01480530bee>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'подготовки'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'Оглядываясь по сторонам, я видел вокруг себя целеустремленных молодых людей')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "iFte_Lpvllpo",
        "outputId": "254f8d9c-d5ad-49fd-8e67-287ed9c580e5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-44d312af6447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Оглядываясь по сторонам, я видел вокруг себя целеустремленных молодых людей'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-d01480530bee>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'оглядываясь'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'Оглядываясь по сторонам, я видел вокруг себя целеустремленных молодых людей, у которых уже имелись начальные ученые степени в сфере бизнеса')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "En6wYe8OllwR",
        "outputId": "113df47e-bf79-4733-b70e-4541f604fa3d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-1b30afd36465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Оглядываясь по сторонам, я видел вокруг себя целеустремленных молодых людей, у которых уже имелись начальные ученые степени в сфере бизнеса'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-d01480530bee>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'оглядываясь'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'За плечами у них уже был опыт аналитической работы в самых престижных фирмах Уолл-стрит. Конечно же, я чувствовал себя не в своей тарелке.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Qhk7J35yllyh",
        "outputId": "26dec819-9078-4340-b597-188585b2108c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-d7167c93cee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'За плечами у них уже был опыт аналитической работы в самых престижных фирмах Уолл-стрит. Конечно же, я чувствовал себя не в своей тарелке.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-d01480530bee>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-90a00363b115>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'аналитической'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод:\n",
        "при замене слоя GRU на BidirectionalGRU судя по лоссу на 2 эпохах качество повысилось. При добавлении еще одного слоя BidirectionalGRU  наоборот ухудшилось. Оставил закомменченным код для энкодера с  GRU. Править пришлось почти все последующие функции. Для сложных предложений выбранного словаря недостаточно."
      ],
      "metadata": {
        "id": "LpmmjzoefdSq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DZ10_nmt.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}